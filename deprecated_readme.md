# Transformer for Imitation Learning
Exploring **decoder-only transformer** models for **imitation learning (IL)**. The models are trained on data generated by reinforcement learning (RL) agents solving the Lunar Lander v3 environment of Gymnasium, a maintained fork of OpenAI’s Gym library, which is an API standard for RL with a diverse collection of environments.

### What is Imitation Learning

Reinforcement learning (RL) approaches require an _**agent**_, an entity that autonomously interacts with an environment to achieve a goal, to learn through trial and error how to perform a task based on a _**reward function**_. A drawback of this approach is that trial-and-error interactions are often time-consuming, and designing reward functions that accurately capture the desired behavior for a given task can be challenging.

Unlike other agent-based learning methods, where an agent learns solely through its own interactions with the environment, imitation learning (IL) leverages the behavior of an _**expert**_ to guide the learning process by mimicking how the expert acts in the environment [1]. In other words, IL frames the agent’s learning task as a supervised learning problem, where experts generate labeled training data in the form of _(x, y)_ pairs where, for example, _x_ represents a snapshot of the environment’s state and _y_ the corresponding action taken by the expert.

The IL approach benefits the agent by allowing it to learn from samples that (theoretically) illustrate successful behaviors, rather than having to discover them through exploration. As a result, IL has attracted considerable attention from researchers, with growing applications in robotics, game playing, and natural language processing [1].

### Transformers

Transformers [2] are a class of Artificial Neural Networks (ANNs) designed to model _**sequential data**_. These networks revolve around a mechanism called _**attention**_, originally proposed as a way to dynamically compute contextualized embeddings in Recurrent Neural Networks (RNNs) for natural language translation tasks [3]. The strength of the Transformer architecture lies in the fact that, thanks to the attention mechanism, elements in a sequence can be processed in parallel (unlike RNNs, which process them sequentially).

Transformers have not only completely revolutionized Natural Language Processing (NLP), becoming the state-of-the-art models for sequence modeling, but have also demonstrated strong performance in other domains such as image classification and computer vision more broadly.

#### Decoder-only Architectures

The original Transformer architecture, introduced for machine translation tasks, consists of two sub-networks: an _**encoder**_, responsible for computing hidden representations of the elements in the input sequence from language _A_, and a _**decoder**_, which uses both the encoder’s representations and its own generated representations for language _B_ to learn how to map one language onto the other.

Since then, derived architectures have emerged that employ only the encoder or only the decoder. Decoder-only models are commonly used for text generation tasks. More precisely, in these models, the representation computed for each token (an element of the sequence) depends _only on its left context_, a mechanism known as **autoregressive** or **causal attention**.

**Autoregressive** models are pretrained to estimate the probability $P(y|x)$ of a sequence of tokens $y = y_1, y_2, ...y_t$ ocurring, given some intial context sequence $x = x_1, x_2, ...x_t$. Since estimating this probability would require evaluating all possible sequences in the target language, what is commonly done is to use the chain rule of probability to factorize it as a product of *conditional* probabilities:

$$ P(y_1,...,y_t|x) =  \prod_{t=1}^{N}P(y_t|y_{<t},x)$$

It is from these conditional probabilities that we can see the *autoregressive* aspect of the task.

# Imitating with Transformers

In imitation learning (IL) problems, a **Markov Decision Process (MDP)**, which is a framework for modeling sequential decision-making, is typically used to represent the environment. Simply put, an MDP defines the transition dynamics _T_ between a **state space** _S_ and an **action space** _A_. In other words, it models situations in which decisions are made sequentially, and the outcomes of actions are uncertain. The goal in IL is for an agent to use the states _S_ and actions _A_ demonstrated by an expert to learn the underlying transition behavior _T_.

When and expert interacts with the environment, it observes the current states $S_{t}$, performs an action $A_{t}$ based on that state, and repeats this process _n_ times until a goal is reached. This generates a **sequence** of state-action pairs ($S_{0}$, $A_{0}$), ($S_{1}$, $A_{1}$), ..., ($S_{t}$, $A_{t}$). Such sequential data naturally lends itself to autoregressive modeling. That is, decoder-only transformers can be trained to estimate the probability

$$ P(A_{t}|S_{t-1}, A_{t-1}, ..., S_{0}) $$

meaning that the model leanrs to predict the next action $A_{t}$ given the previously observed sequence of states and actions, similar to how text generation models predict the next word in a sentence.

# Experimental Setup

## Data and dataset generation

We need expert data to perform our experiments. Running `scripts/run_expert_data_gen.py` simulates multiple episodes of the Lunar Lander v3 environment with a trained actor-critic model controling it, and the raw data is saved within `data/raw`.

Once the raw expert data exists we can run `scripts/run_dataset_gen.py`, which uses this data to create a custom `torch.nn.Dataset` comprising of pairs of sequences (intercalated state space vectors and actions) and labels (actions to be taken given a specific sequence). This dataset is saved within `data/processed`.

## Setting up the environment

This project was developed using a conda environment. All the packages names and versions necessary to run the scripts are detailed in the `environment.yml` file. Run `conda create -f environment.yml` to it setup.

## References

[1] Nathan Gavenski, Felipe Meneguzzi, Michael Luck, & Odinaldo Rodrigues. (2024). A Survey of Imitation Learning Methods, Environments and Metrics.
[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, & Illia Polosukhin. (2023). Attention Is All You Need.
[3] Dzmitry Bahdanau, Kyunghyun Cho, & Yoshua Bengio. (2016). Neural Machine Translation by Jointly Learning to Align and Translate.